# SimpleNeuralNetwork
Нйросеть которая ничего не делает

## Скелет кода
* Инициализация - задание количества входных, скрытых и выходных узлов.

* Тренировка - уточнение весовых коэффициентов в процессе обработки предоставленных для обучения сети тренировочных примеров.

* Опрос - получение значений сигналов с выходных узлов после предоставления значений входящих сигналов.

## Весовые коэффициенты - сердце сети

Весовые коэфициенты связей используются для расчета распространения сигналов в прямом направлении, а также обратного распространения ошибок.


* Матрица весов для связей между входным и скрытым слоями, w(входной_скрытый), размерностью hidden_nodes X input_nodes.

* Матрица для связей между скрытым и выходным слоями, w(скрытый_выходной), размерностью output_nodes X hidden_nodes.

Следующая функция из пакета *numpy* генерирует массив случайных чисел в диапазоне от 0 до 1, где размерность массива равна rows X columns
В программе отнимаем 0.5 что бы диапазон был от -1.0 до + 1.0

```
numpy.random.rand(rows, columns)
```

Небольшое усовершенствование 
```
numpy.random.normal(0.0, pow(self.hnodes, -0.5), self.hnodes, self.inodes)
```
позволит создавать случайные числа из нормального распределения с центром в нуле и со стандартным отклонением, величина которого обратно пропорциональна корню квадратному из количесва входящих связей на узел.

## Опрос сети

Функция *query()* принимает в качестве аргумента входные данные нейронной сети и возвращает ее выходные данные.

Ниже показано, как можно получить входящие сигналы для узлов скрытого слоя путем сочетания матрицы весовых коэффициентов связей между входным и скрытым слоями с матрицей выходных сигналов:
```
X(скрытый) = W(входной_скрытый) * I
```

Ниже представлена инструкция, которая показывает, как применить функцию скалярного произведения библиотеки *numpy* к матрицам весов и входных сигналов:
```
hidden_inputs = numpy.dot(self.wih, inputs)
```

Для получения выходных сигналов скрытого слоя мы просто применяем к каждому из них сигмоиду:
```
O(скрытый) = сигмоида(X(скрытый))
```

Ниже приведен код, определяющий функцию активации(фукнция сигмоиды в библиотеке *scipy*):
```
self.activation_function = lambda x: scipy.cpecial.expit(x)
```

Применяем функцию активации к сглаженным комбинированным входящим сигналам, поступающим на скрытые узлы:
```
hidden_outputs = self.activation_function(hidden_inputs)
```
Таким образом, сигналы, исходящие из скрытого слоя, описываются матрицей *hidden_outputs*.

## Тренировка сети

* Расчет выходных сигналов для заданного тренировачного примера.

* Сравнение рассчитанных выходныз сигналов с желаемым ответом и обновление весовых коэффициентов связей между узлами на основе найденных различий.

Вычислить ошибку, являющуюся разностью между желаемым и целевым значением, предоставленным тренировочным примером, и фактически выходным значением(*targets - final_outputs*).

Далее мы должны рассчитать обратное распространение ошибок для узлов скрытого слоя.
```
ошибки(скрытый) = веса(скрытый_выходной).Т Х ошибки(выходной)
```

Выражение для обновления веса связи между узлом j и узлом k следующего слоя в матричной форме

![формула](https://i.ibb.co/pKQ0R8b/nn.jpg)

Величина a - это коэффициент обучения, а сигмойда - это функция активации.
* = обычное умножение.
. = скалярное.

на python:
```
self.who += self.lr * numpy.dot((outputs_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))
```
